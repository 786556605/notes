### **Chapter 15. The Process Address Space**

[Chapter 12 Memory Management](ch12.md) covers how the kernel manages physical memory.  In addition to managing its own memory, the kernel also has to manage the memory of user-space processes. This memory is called the **process address space**, which is the representation of memory given to each user-space process on the system.

Linux is a virtual memory operating system:

* The memory is virtualized among the processes on the system. An individual process’s view of memory is as if it alone has full access to the system’s physical memory.
* More important, the address space of even a single process can be much larger than physical memory.

This chapter discusses how the kernel manages the process address space.

### Address Spaces

#### Flat and segmented address space *

The process address space consists of the virtual memory addressable by a process and the addresses within the virtual memory that the process is allowed to use. Each process is given a flat 32- or 64-bit address space, with the size depending on the architecture. The term *flat* denotes that the address space exists in a single range. (For example, a 32-bit address space extends from the address 0 to 4294967295.)

Some operating systems provide a **segmented address space**, with addresses existing not in a single linear range, but instead in multiple segments. Modern virtual memory operating systems generally have a flat memory model and not a segmented one.

In the flat memory model, the flat address space is unique to each process. A memory address in one process’s address space is completely unrelated to that same memory address in another process’s address space. Both processes can have different data at the same address in their respective address spaces.Alternatively, processes can elect to share their address space with other processes; these processes are known as as **threads**.

#### Memory areas*

A memory address is a given value within the address space, such as `4021f000`. This particular value identifies a specific byte in a process’s 32-bit address space. Although a process can address up to 4GB of memory (with a 32-bit address space), it doesn’t have permission to access all of it.

These intervals of legal addresses (the process has permission to access), such as `08048000-0804c000` are called **memory areas**. The process, through the kernel, can dynamically add and remove memory areas to its address space.

The process can access a memory address only in a valid memory area. Memory areas have associated permissions, such as readable, writable, and executable, that the associated process must respect. If a process accesses a memory address not in a valid memory area, or if it accesses a valid area in an invalid manner, the kernel kills the process with the dreaded "Segmentation Fault" message.

Memory areas can contain the following things:

* A memory map of the executable file’s code, called the **text section**.
* A memory map of the executable file’s initialized global variables, called the **data section**.
* A memory map of the zero page (a page consisting of all zeros, used for purposes such as this) containing uninitialized global variables, called the **bss section**.
    * The term "BSS" is historical and stands for **b**lock **s**tarted by **s**ymbol.
    * Uninitialized variables are not stored in the executable object because they do not have any associated value. But the C standard decrees that uninitialized global variables are assigned certain default values (all zeros), so the kernel loads the variables (without value) from the executable into memory and maps the zero page over the area, thereby giving the variables the value zero, without having to waste space in the object file with explicit initializations.
* A memory map of the zero page used for the process’s user-space stack. (Do not confuse this with the process’s [kernel stack](ch12.md#statically-allocating-on-the-stack), which is separate and maintained and used by the kernel.)
* An additional text, data, and bss section for each shared library, such as the C library and dynamic linker, loaded into the process’s address space.
* Any memory mapped files.
* Any shared memory segments.
* Any anonymous memory mappings, such as those associated with `malloc()` (newer versions of glibc implement `malloc()` via `mmap()`, in addition to `brk()`).

### The Memory Descriptor

The kernel represents a process’s address space with a data structure called the **memory descriptor**. This structure contains all the information related to the process address space.

The memory descriptor is represented by `struct mm_struct` and defined in `<linux/mm_types.h>` ([include/linux/mm_types.h](https://github.com/shichao-an/linux/blob/v2.6.34/include/linux/mm_types.h)):

<small>[include/linux/mm_types.h#L222](https://github.com/shichao-an/linux/blob/v2.6.34/include/linux/mm_types.h#L222)</small>

```c
struct mm_struct {
    struct vm_area_struct *mmap; /* list of memory areas */
    struct rb_root mm_rb; /* red-black tree of VMAs */
    struct vm_area_struct *mmap_cache; /* last used memory area */
    unsigned long free_area_cache; /* 1st address space hole */
    pgd_t *pgd; /* page global directory */
    atomic_t mm_users; /* address space users */
    atomic_t mm_count; /* primary usage counter */
    int map_count; /* number of memory areas */
    struct rw_semaphore mmap_sem; /* memory area semaphore */
    spinlock_t page_table_lock; /* page table lock */
    struct list_head mmlist; /* list of all mm_structs */
    unsigned long start_code; /* start address of code */
    unsigned long end_code; /* final address of code */
    unsigned long start_data; /* start address of data */
    unsigned long end_data; /* final address of data */
    unsigned long start_brk; /* start address of heap */
    unsigned long brk; /* final address of heap */
    unsigned long start_stack; /* start address of stack */
    unsigned long arg_start; /* start of arguments */
    unsigned long arg_end; /* end of arguments */
    unsigned long env_start; /* start of environment */
    unsigned long env_end; /* end of environment */
    unsigned long rss; /* pages allocated */
    unsigned long total_vm; /* total number of pages */
    unsigned long locked_vm; /* number of locked pages */
    unsigned long saved_auxv[AT_VECTOR_SIZE]; /* saved auxv */
    cpumask_t cpu_vm_mask; /* lazy TLB switch mask */
    mm_context_t context; /* arch-specific data */
    unsigned long flags; /* status flags */
    int core_waiters; /* thread core dump waiters */
    struct core_state *core_state; /* core dump support */
    spinlock_t ioctx_lock; /* AIO I/O list lock */
    struct hlist_head ioctx_list; /* AIO I/O list */
};
```

* The `mm_users` field is the number of processes using this address space.
    * For example, if two threads share this address space, `mm_users` is equal to two.
* The `mm_count` field is the primary reference count for the `mm_struct`.
    * All `mm_users` equate to one increment of `mm_count`. In the previous example, `mm_count` is only one. If nine threads shared an address space, `mm_users` would be nine, but again `mm_count` would be only one. Only when `mm_users` reaches zero (when all threads using an address space exit) is mm_count decremented. When `mm_count` finally reaches zero, there are no remaining references to this `mm_struct`, and it is freed.
    * When the kernel operates on an address space and needs to bump its associated reference count, the kernel increments `mm_count`.

Having two counters enables the kernel to differentiate between the main usage counter (`mm_count`) and the number of processes using the address space (`mm_users`).

* The `mmap` and `mm_rb` fields are different data structures that contain the same thing: all the memory areas in this address space.
    * `mmap` stores them in a linked list.
    * `mm_rb` stores them in a red-black tree. A red-black tree is a type of binary tree; like all binary trees, searching for a given element is an O(log *n*) operation.

This redundancy (two `mmap` and `mm_rb` data structures on the same data) are handy:

* The `mmap` data structure, as a linked list, allows for simple and efficient traversing of all elements.
* The `mm_rb` data structure, as a red-black tree, is more suitable to searching for a given element.

The kernel isn’t duplicating the `mm_struct` structures; just the containing objects. Overlaying a linked list onto a tree, and using both to access the same set of data, is sometimes called a [**threaded tree**](https://en.wikipedia.org/wiki/Threaded_binary_tree).

All of the `mm_struct` structures are strung together in a doubly linked list via the `mmlist` field. The initial element in the list is the `init_mm` memory descriptor, which describes the address space of the `init` process. The list is protected from concurrent access via the `mmlist_lock`, which is defined in [kernel/fork.c](https://github.com/shichao-an/linux/blob/v2.6.34/kernel/fork.c).

#### Allocating a Memory Descriptor

The memory descriptor associated with a given task is stored in the `mm` field of the task’s process descriptor:

* The process descriptor is represented by the `task_struct` structure, defined in `<linux/sched.h>`. Thus, `current->mm` is the current process’s memory descriptor.
* The `copy_mm()` function copies a parent’s memory descriptor to its child during `fork()`.
* The `mm_struct` structure is allocated from the `mm_cachep` slab cache via the `allocate_mm()` macro in [kernel/fork.c](https://github.com/shichao-an/linux/blob/v2.6.34/kernel/fork.c).
* Each process receives a unique `mm_struct` and thus a unique process address space.

Processes may share their address spaces with their children via the `CLONE_VM` flag to `clone()`. The process is then called a *thread*. <u>This is essentially the only difference between normal processes and so-called threads in Linux ([Chapter 3](ch3.md)); the Linux kernel does not otherwise differentiate between them. Threads are regular processes to the kernel that merely share certain resources.</u>

When `CLONE_VM` is specified, `allocate_mm()` is not called, and the process’s `mm` field is set to point to the memory descriptor of its parent via this logic in `copy_mm()`:

```c
if (clone_flags & CLONE_VM) {
    /*
    * current is the parent process and
    * tsk is the child process during a fork()
    */
    atomic_inc(&current->mm->mm_users);
    tsk->mm = current->mm;
}
```

#### Destroying a Memory Descriptor

When the process associated with a specific address space exits, `exit_mm()`, defined in [kernel/exit.c](https://github.com/shichao-an/linux/blob/v2.6.34/kernel/exit.c), is invoked. This function performs some housekeeping and updates some statistics. It then calls `mmput()`, which decrements the memory descriptor’s `mm_users` user counter. If the user count reaches zero, `mmdrop()` is called to decrement the `mm_count` usage counter. If that counter is finally zero, the `free_mm()` macro is invoked to return the `mm_struct` to the `mm_cachep` slab cache via `kmem_cache_free()`, because the memory descriptor does not have any users.

#### The `mm_struct` and Kernel Threads

<u>Kernel threads do not have a process address space and therefore do not have an associated memory descriptor.</u> Thus, the `mm` field of a kernel thread’s process descriptor is `NULL`.  This is the definition of a kernel thread: processes that have no user context.

Kernel threads do not ever access any userspace memory. Because kernel threads do not have any pages in user-space, they do not deserve their own memory descriptor and [page tables](#page-tables) (discussed later in the chapter). However, kernel threads need some of the data, such as the page tables, even to access kernel memory. To provide kernel threads the needed data, without wasting memory on a memory descriptor and page tables, or wasting processor cycles to switch to a new address space whenever a kernel thread begins running, kernel threads use the memory descriptor of whatever task ran previously:

* Whenever a process is scheduled, the process address space referenced by the process’s `mm` field is loaded. The `active_mm` field in the process descriptor is then updated to refer to the new address space.
* Kernel threads do not have an address space and `mm` is `NULL`. Therefore, when a kernel thread is scheduled, the kernel notices that `mm` is `NULL` and keeps the previous process’s address space loaded. The kernel then updates the `active_mm` field of the kernel thread’s process descriptor to refer to the previous process’s memory descriptor.
* The kernel thread can then use the previous process’s page tables as needed. Because kernel threads do not access user-space memory, they make use of only the information in the address space pertaining to kernel memory, which is the same for all processes.

### Virtual Memory Areas

The memory area structure `vm_area_struct` represents memory areas, defined in `<linux/mm_types.h>`. In the Linux kernel, memory areas are often called **virtual memory areas** (**VMA**s).

The `vm_area_struct` structure describes a single memory area over a contiguous interval in a given address space:

* The kernel treats each memory area as a unique memory object.
* Each memory area possesses certain properties, such as permissions and a set of associated operations.

In this manner, each VMA structure can represent different types of memory areas, e.g. memory-mapped files or the process’s user-space stack. This is similar to the object-oriented approach taken by the VFS layer ([Chapter 13](ch13.md)).

<small>[include/linux/mm_types.h#L130](https://github.com/shichao-an/linux/blob/v2.6.34/include/linux/mm_types.h#L130)</small>

```c
struct vm_area_struct {
    struct mm_struct *vm_mm; /* associated mm_struct */
    unsigned long vm_start; /* VMA start, inclusive */
    unsigned long vm_end; /* VMA end , exclusive */
    struct vm_area_struct *vm_next; /* list of VMA’s */
    pgprot_t vm_page_prot; /* access permissions */
    unsigned long vm_flags; /* flags */
    struct rb_node vm_rb; /* VMA’s node in the tree */
    union { /* links to address_space->i_mmap or i_mmap_nonlinear */
    struct {
            struct list_head list;
            void *parent;
            struct vm_area_struct *head;
        } vm_set;
        struct prio_tree_node prio_tree_node;
    } shared;
    struct list_head anon_vma_node; /* anon_vma entry */
    struct anon_vma *anon_vma; /* anonymous VMA object */
    struct vm_operations_struct *vm_ops; /* associated ops */
    unsigned long vm_pgoff; /* offset within file */
    struct file *vm_file; /* mapped file, if any */
    void *vm_private_data; /* private data */
};
```

Each memory descriptor is associated with a unique interval in the process’s address space.

* The `vm_start` field is the initial (lowest) address in the interval. (inclusive start)
* The `vm_end` field is the first byte after the final (highest) address in the interval. (exclusive end)

`vm_end - vm_start` is the length in bytes of the memory area, which exists over the interval `[vm_start, vm_end)`. Intervals in different memory areas in the same address space cannot overlap.

The `vm_mm` field points to this VMA’s associated `mm_struct`. Each VMA is unique to the `mm_struct` with which it is associated:

* Therefore, even if two separate processes map the same file into their respective address spaces, each has a unique `vm_area_struct` to identify its unique memory area.
* Conversely, two threads that share an address space also share all the `vm_area_struct` structures therein.

#### VMA Flags

The `vm_flags` field contains bit flags, defined in `<linux/mm.h>`, that specify the behavior of and provide information about the pages contained in the memory area. Unlike permissions associated with a specific physical page, the VMA flags specify behavior for which the kernel is responsible, not the hardware. `vm_flags` contains information that relates to the memory area as a whole (each page), not specific individual pages.

The following table is a listing of the possible `vm_flags` values.

Flag | Effect on the VMA and Its Pages
---- | -------------------------------
`VM_READ` | Pages can be read from.
`VM_WRITE` | Pages can be written to.
`VM_EXEC` | Pages can be executed.
`VM_SHARED` | Pages are shared.
`VM_MAYREAD` | The `VM_READ` flag can be set.
`VM_MAYWRITE` | The `VM_WRITE` flag can be set.
`VM_MAYEXEC` | The `VM_EXEC` flag can be set.
`VM_MAYSHARE` | The `VM_SHARE` flag can be set.
`VM_GROWSDOWN` | The area can grow downward.
`VM_GROWSUP` | The area can grow upward.
`VM_SHM` | The area is used for shared memory.
`VM_DENYWRITE` | The area maps an unwritable file.
`VM_EXECUTABLE` | The area maps an executable file.
`VM_LOCKED` | The pages in this area are locked.
`VM_IO` | The area maps a device’s I/O space.
`VM_SEQ_READ` | The pages seem to be accessed sequentially.
`VM_RAND_READ` | The pages seem to be accessed randomly.
`VM_DONTCOPY` | This area must not be copied on `fork()`.
`VM_DONTEXPAND` | This area cannot grow via `mremap()`.
`VM_RESERVED` | This area must not be swapped out.
`VM_ACCOUNT` | This area is an accounted VM object.
`VM_HUGETLB` | This area uses hugetlb pages.
`VM_NONLINEAR` | This area is a nonlinear mapping.

* The `VM_READ`, `VM_WRITE`, and `VM_EXEC` flags specify the usual read, write, and execute permissions for the pages in this particular memory area. They are combined as needed to form the appropriate access permissions that a process accessing this VMA must respect. For example:
    * The object code for a process might be mapped with `VM_READ` and `VM_EXEC` but not `VM_WRITE`.
    * The data section from an executable object would be mapped `VM_READ` and `VM_WRITE`, but `VM_EXEC` would make little sense.
    * A read-only memory mapped data file would be mapped with only the `VM_READ` flag.
* The `VM_SHARED` flag specifies whether the memory area contains a mapping that is shared among multiple processes.
    * If the flag is set, it is intuitively called a **shared mapping**.
    * If the flag is not set, only a single process can view this particular mapping, and it is called a **private mapping**.
* The `VM_IO` flag specifies that this memory area is a mapping of a device’s I/O space. This field is typically set by device drivers when `mmap()` is called on their I/O space. It specifies, among other things, that the memory area must not be included in any process’s core dump.
* The `VM_RESERVED` flag specifies that the memory region must not be swapped out. It is also used by device driver mappings.
* The `VM_SEQ_READ` flag indicates that the application is performing sequential (i.e. linear and contiguous) reads in this mapping. <u>The kernel can then opt to increase the read-ahead performed on the backing file.</u>
* The `VM_RAND_READ` flag specifies that the application is performing relatively random (i.e. not sequential) reads in this mapping. The kernel can then opt to decrease or disable read-ahead on the backing file.

These flags (`VM_SEQ_READ` and `VM_RAND_READ` flags) are set via the `madvise()` system call with the `MADV_SEQUENTIAL` and `MADV_RANDOM` flags, respectively:

* Read-ahead is the act of reading sequentially ahead of requested data, in hopes that the additional data will be needed soon. This is beneficial if applications are reading data sequentially.
* If data access patterns are random, however, read-ahead is not effective.

#### VMA Operations

The `vm_ops` field in the `vm_area_struct` structure points to the table of operations associated with a given memory area, which the kernel can invoke to manipulate the VMA. The `vm_area_struct` acts as a generic object for representing any type of memory area, and the operations table describes the specific methods that can operate on this particular instance of the object.

The operations table is represented by struct `vm_operations_struct` and is defined in `<linux/mm.h>`:

<small>[include/linux/mm.h#L185](https://github.com/shichao-an/linux/blob/v2.6.34/include/linux/mm.h#L185)</small>

```c
struct vm_operations_struct {
    void (*open) (struct vm_area_struct *);
    void (*close) (struct vm_area_struct *);
    int (*fault) (struct vm_area_struct *, struct vm_fault *);
    int (*page_mkwrite) (struct vm_area_struct *vma, struct vm_fault *vmf);
    int (*access) (struct vm_area_struct *, unsigned long ,
    void *, int, int);
};
```

* `void open(struct vm_area_struct *area)`. This function is invoked when the given memory area is added to an address space.
* `void close(struct vm_area_struct *area)`. This function is invoked when the given memory area is removed from an
address space.
* `int fault(struct vm_area_sruct *area, struct vm_fault *vmf)`. This function is invoked by the page fault handler when a page that is not present in physical memory is accessed.
* `int page_mkwrite(struct vm_area_sruct *area, struct vm_fault *vmf)`. This function is invoked by the page fault handler when a page that was read-only is being made writable.
* `int access(struct vm_area_struct *vma, unsigned long address, void *buf, int len, int write)`. This function is invoked by `access_process_vm()` when `get_user_pages()` fails.

#### Lists and Trees of Memory Areas

As discussed earlier, memory areas are accessed via both the `mmap` and the `mm_rb` fields of the memory descriptor. These two data structures independently point to all the memory area objects associated with the memory descriptor. They both contain pointers to the same `vm_area_struct` structures, represented in different ways.

The first field, `mmap`, links together all the memory area objects in a singly linked list:

* Each `vm_area_struct` structure is linked into the list via its `vm_next` field.
* The areas are sorted by ascending address.
* The first memory area is the `vm_area_struct` structure to which mmap points.
* The last structure points to `NULL`.

The second field, `mm_rb`, links together all the memory area objects in a red-black tree.

* The root of the red-black tree is `mm_rb`
* Each `vm_area_struct` structure in this address space is linked to the tree via its `vm_rb` field.

A [**red-black tree**](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree) is a type of balanced binary tree.

* Each element in a red-black tree is called a node.
* The initial node is called the root of the tree.
* Most nodes have two children: a left child and a right child. Some nodes have only one child, and the final nodes, called leaves, have no children. For any node, the elements to the left are smaller in value, whereas the elements to the right are larger in value. Furthermore, each node is assigned a color (red or black) according to two rules: The children of a red node are black, and every path through the tree from a node to a leaf must contain the same number of black nodes. The root node is always red.
* Searching of, insertion to, and deletion from the tree is an O(log(*n*)) operation.

The kernel uses the redundant data structures to provide optimal performance regardless of the operation performed on the memory areas:

* The linked list is used when every node needs to be traversed.
* The red-black tree is used when locating a specific memory area in the address space.
