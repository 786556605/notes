### **Chapter 12. TCP: The Transmission Control Protocol (Preliminaries)**

### Introduction

[p579]

The protocols discussed so far do not include mechanisms for delivering data reliably; they may detect that erroneous data has been received, using a checksum or CRC, but they do not try very hard to repair errors:

* With IP and UDP, no error repair is done at all.
* With Ethernet and other protocols based on it, the protocol provides some number of retries and then gives up if it cannot succeed.

##### **Information theory and coding theory**

* [**Error-correcting codes**](https://en.wikipedia.org/wiki/Error_detection_and_correction#Error-correcting_code) (adding redundant bits so that the real information can be retrieved even if some bits are damaged) is one way to correct communications problems is one very important method for handling errors.
* [**Automatic Repeat Request**](https://en.wikipedia.org/wiki/Error_detection_and_correction#Automatic_repeat_request_.28ARQ.29) (ARQ): which means "try sending again" until the information is finally received. This approach forms the basis for many communications protocols, including TCP.

#### ARQ and Retransmission

For a multihop communications channel, there are other problems besides packet bit errors:

* Problems that arise at an intermediate router
* Packet reordering
* Packet duplication
* Packet erasures (drops)

An error-correcting protocol designed for use over a multihop communications channel (such as IP) must cope with all of these problems.

##### **Packet drops and bit errors**

A straightforward method dealing with packet drops (and bit errors) is to resend the packet until it is received properly. This requires a way to determine:

1. **Whether the receiver has received the packet.**
2. **Whether the packet it received was the same one the sender sent.**

This is solved by using acknowledgment (ACK): the sender sends a packet and awaits an ACK. When the receiver receives
the packet, it sends the ACK. When the sender receives the ACK, it sends another
packet, and the process continues. Interesting questions to ask here are:

1. **How long should the sender (expect to) wait for an ACK?**
    * This is discussed in [Chapter 14](ch14.md).
2. **What if the ACK is lost?**
    * If an ACK is dropped, the sender cannot distinguish this case from the case in which the original packet is dropped, so it simply sends the packet again. The receiver may receive two or more copies in that case, so it must be prepared to handle that situation
3. **What if the packet was received but had errors in it?**
    * Detecting errors is easiter than correcting errors. By using a form of checksum. When a receiver receives a packet containing an error, it refrains from sending an ACK. Eventually, the sender resends the packet, which ideally arrives undamaged.

##### **Packet duplication**

The receiver might receive duplicate copies of the packet. This problem is addressed using a **sequence number**. Every unique packet gets a new sequence number when it is sent at the source, and this sequence number is carried along in the packet itself. The receiver can use this number to determine whether it has already seen the packet and if so, discard it.

##### **Efficiency**

The protocol described so far is reliable but not very efficient. The sender injects a single packet into the communications path but then must stop until it hears the ACK. This protocol is therefore called "**stop and wait**". Its throughput performance (data sent on the network per unit time) is proportional to *M/R* where *M* is the packet size and *R* is the round-trip time (RTT), assuming no packets are lost or irreparably damaged in transit. For a fixed-size packet, as *R* goes up, the throughput goes down. If packets are lost or damaged, the situation is even worse: the "[goodput](https://en.wikipedia.org/wiki/Goodput)" (useful amount of data transferred per unit time) can be considerably less than the throughput.

For a network that doesn’t damage or drop many packets, the cause for low throughput is usually that the network is not being kept busy. The situation is similar to using an assembly line where new work cannot enter the line until a complete product emerges. Most of the line goes idle. We could have more than one work unit in the line at a time. This is same for networks: if we could have more than one packet in the network, we would keep it "more busy", leading to higher throughput.

Allowing more than one packet to be in the network at a time:

* The sender must decide not only when to inject a packet into the network, but also how many. It also must figure out how to keep the timers when waiting for ACKs, and it must keep a copy of each packet not yet acknowledged in case retransmissions are necessary.
* The receiver needs to have a more sophisticated ACK mechanism: one that can distinguish which packets have been received and which have not.
* The receiver may need a more sophisticated buffering (packet storage) mechanism: one that allows it to hold "out-of-sequence" packets (those packets that have arrived earlier than those expected because of loss or reordering).

There are other issues:

* What if the receiver is slower than the sender? If the sender simply injects many packets at a very high rate, the receiver might just drop them because of processing or memory limitations. The same question can be asked about the routers in the middle.
* What if the network infrastructure cannot handle the rate of data the sender and receiver wish to use?

#### Windows of Packets and Sliding Windows

Assume each unique packet has a sequence number. We define a **window** of packets as the collection of packets (or their sequence numbers) that have been injected by the sender but not yet completely acknowledged (the sender has not received an ACK for them). We refer to the **window size** as the number of packets in the window.

[![The sender’s window, showing which packets are eligible to be sent (or have already been sent), which are not yet eligible, and which have already been sent and acknowledged. In this example, the window size is fixed at three packets.](figure_12-1_600.png)](figure_12-1.png " The sender’s window, showing which packets are eligible to be sent (or have already been sent), which are not yet eligible, and which have already been sent and acknowledged.  In this example, the window size is fixed at three packets.")

In the figure:

* Packet number 3 has already been sent and acknowledged, so the copy of it that the sender was keeping can now be released.
* Packet 7 is ready at the sender but not yet able to be sent because it is not yet "in" the window.
  * When the sender receives an ACK for packet 4, the window "slides" to the right by one packet, meaning that the copy of packet 4 can be released and packet 7 can be sent.

This movement of the window gives rise to another name for this type of protocol, a **sliding window** protocol.

Typically, this window structure is kept at both the sender and the receiver.

* At the sender, it keeps track of what packets can be released, awaiting ACKs, and cannot yet be sent.
* At the receiver, it keeps track of:
    * What packets have already been received and acknowledged,
    * What packets are expected (and how much memory has been allocated to hold them),
    * Which packets (even if received) will not be kept because of limited memory.

Although the window structure is convenient for keeping track of data as it flows between sender and receiver, it does not provide guidance as to how large the window should be, or what happens if the receiver or network cannot handle the sender’s data rate.

#### Variable Windows: Flow Control and Congestion Control

**Flow control** can handle problem that arises when a receiver is too slow relative to a sender, by forcing the sender to slow down when the receiver cannot keep up. It is usually handled in one of two ways:

* **Rate-based flow control** gives the sender a certain data rate allocation and ensures that data is never allowed to be sent at a rate that exceeds the allocation. This type of flow control is most appropriate for streaming applications and can be used with broadcast and multicast delivery (Chapter 9).
* **Window-based flow control** is the most popular approach when sliding windows are being used. In this approach, the window size is not fixed but is instead allowed to vary over time.
    * **Window advertisement**, or simply a **window update** is a method for the receiver to signal the sender how large a window to use. This value is used by the sender (the receiver of the window advertisement) to adjust its window size.
    * Logically, window update is separate from the ACKs we discussed previously, but in practice the window update and ACK are carried in a single packet, meaning that the sender tends to adjust the size of its window at the same time it slides it to the right.

If we consider the effect of changing the window size at the sender, it becomes clear how this achieves flow control. The sender is allowed to inject *W* packets into the network before it hears an ACK for any of them. If the sender and receiver are sufficiently fast, and the network loses no packets and has an infinite capacity, this means that the transfer rate is proportional to (*SW/R*) bits/s, where *W* is the window size, *S* is the packet size in bits, and *R* is the RTT. When the window advertisement from the receiver clamps the value of *W* at the sender, the sender’s overall rate can be limited so as to not overwhelm the receiver.

This approach works fine for protecting the receiver, but what about the network in between? We may have routers with limited memory between the sender and the receiver that have to contend with slow network links. When this happens, it is possible for the sender’s rate to exceed a router’s ability to keep up, leading to packet loss. This is addressed with a special form of flow control called **congestion control**.

Congestion control involves the sender slowing down so as to not overwhelm the network between itself and the receiver.

* **Explicit signaling** uses a window advertisement to signal the sender to slow down for the receiver.
* **Implicit signaling**: the sender guesses that it needs to slow down. It would involve deciding to slow down based on some other evidence.

The problem of congestion control in datagram-style networks, and more generally **queuing theory** to which it is closely related, has remained a major research topic for years, and it is unlikely to ever be solved completely for all circumstances. It is also not practical to discuss all the options and methods of performing flow control here. In Chapter 16 we will explore the particular congestion control technique used with TCP in more detail, along with a number of variants that have arisen over the years.

#### Setting the Retransmission Timeout

One of the most important performance issues is how long to wait before concluding that a packet has been lost and should be resent. That is, **What should the retransmission timeout be?** Intuitively, the amount of time the sender should wait before
resending a packet is about the sum of the following times:

1. The time to send the packet,
2. The time for the receiver to process it and send an ACK,
3. The time for the ACK to travel back to the sender,
4. The time for the sender to process the ACK.

In practice, none of these times are known with certainty and any or all of them vary over time as additional load is added to or removed from the end hosts or routers.

Because it is not practical for the user to estimate all the times, a better strategy is to have the protocol implementation try to estimate them. This is called **round-trip-time estimation** and is a statistical process. The true RTT is likely to be close to the sample mean of a collection of samples of RTTs. This average naturally changes over time (it is not stationary), as the paths taken through the network may change.

[p584]

It would not be sensible to set the retransmission timer to be exactly equal to the mean estimator, as it is likely that many actual RTTs will be larger, thereby inducing unwanted retransmissions.

* The timeout should be set to something larger than the mean, but exactly what this relationship is (or even if the mean should be directly used) is not yet clear.
* Setting the timeout too large is also undesirable, as this leads back to letting the network go idle, reducing throughput.

This is further explored in Chapter 14.

### Introduction to TCP

Our description of TCP starts in this chapter and continues in the next five chapters:

* Chapter 13: how a TCP connection is established and terminated.
* Chapter 14:
    * How TCP estimates the per-connection RTT.
    * How the retransmission timeout is set based on the above estimate.
* Chapter 15:
    * Normal transfer of data (starting with "interactive" applications, such as chat).
    * Window management and flow control, which apply to both interactive and "bulk" data flow applications (such as file transfer).
    * **Urgent mechanism** of TCP, which allows a sender to mark certain data in the data stream as special.
* Chapter 16:
    * Congestion control algorithms in TCP that help to reduce packet loss when the network is very busy.
    * Modifications that have been proposed to increase throughput on fast networks or improve resiliency on lossy (e.g., wireless) networks.
* Chapter 17: how TCP keeps connections active even when no data is flowing.

#### The TCP Service Model

Even though TCP and UDP use the same network layer (IPv4 or IPv6), TCP provides a totally different service to the application layer from what UDP does. <u>TCP provides a **connection-oriented**, reliable, byte stream service.</u> The term connection-oriented means that the two applications using TCP must establish a TCP connection by contacting each other before they can exchange data. There are exactly two endpoints communicating with each other on a TCP connection; concepts such as broadcasting and multicasting (Chapter 9) are not applicable to TCP.

TCP provides a byte stream abstraction to applications that use it. Its consequence is that no record markers or message boundaries are automatically inserted by TCP ([Chapter 1](ch1.md#message-boundaries)). A record marker corresponds to an indication of an application’s write extent. If the application on one end writes 10 bytes, followed by a write of 20 bytes, followed by a write of 50 bytes, the application at the other end of the connection cannot tell what size the individual writes were. For example, the other end may read the 80 bytes in four reads of 20 bytes at a time or in some other way. One end puts a stream of bytes into TCP, and the identical stream of bytes appears at the other end. Each endpoint individually chooses its read and write sizes.

TCP does not interpret the contents of the bytes in the byte stream at all. It has no idea if the data bytes being exchanged are binary data, ASCII characters, [EBCDIC](https://en.wikipedia.org/wiki/EBCDIC) characters, or something else. The interpretation of this byte stream is up to the applications on each end of the connection. TCP supports the urgent mechanism mentioned before, although it is no longer recommended for use.

#### Reliability in TCP

TCP provides reliability using specific variations on the techniques just described.

* **Packetization**: TCP must convert a sending application’s stream of bytes into a set of packets that IP can carry, since it provides a byte stream interface.
* **Repacketization**: these packets contain sequence numbers, which in TCP actually represent the byte offsets of the first byte in each packet in the overall data stream rather than packet numbers. This allows packets to be of variable size during a transfer and may also allow them to be combined.

The application data is broken into what TCP considers the best-size chunks to send, typically fitting each segment into a single IP-layer datagram that will not be fragmented.  This is different from UDP, where each write by the application usually generates a UDP datagram of that size (plus headers). The chunk passed by TCP to IP is called a **segment**. Chapter 15 discusses how TCP decides what size a segment should be.

### TCP Header and Encapsulation

TCP is encapsulated in IP datagrams as shown the figure below:

[![Figure 12-2 The TCP header appears immediately following the IP header or last IPv6 extension header and is often 20 bytes long (with no TCP options). With options, the TCP header can be as large as 60 bytes. Common options include Maximum Segment Size, Timestamps, Window Scaling, and Selective ACKs.](figure_12-2_600.png)](figure_12-2.png "Figure 12-2 The TCP header appears immediately following the IP header or last IPv6 extension header and is often 20 bytes long (with no TCP options). With options, the TCP header can be as large as 60 bytes. Common options include Maximum Segment Size, Timestamps, Window Scaling, and Selective ACKs.")

The TCP header (show in the figure below) is considerably more complicated than the UDP header ([Chapter 10](ch10.md)). This is not very surprising, as TCP is a significantly more complicated protocol that must keep each end of the connection informed (synchronized) about the current state.

[![Figure 12-3 The TCP header. Its normal size is 20 bytes, unless options are present. The Header Length field gives the size of the header in 32-bit words (minimum value is 5). The shaded fields (Acknowledgment Number, Window Size, plus ECE and ACK bits) refer to the data flowing in the opposite direction relative to the sender of this segment.](figure_12-3_600.png)](figure_12-3.png "Figure 12-3 The TCP header. Its normal size is 20 bytes, unless options are present. The Header Length field gives the size of the header in 32-bit words (minimum value is 5). The shaded fields (Acknowledgment Number, Window Size, plus ECE and ACK bits) refer to the data flowing in the opposite direction relative to the sender of this segment.")

Each TCP header contains the source and destination port number. <u>The source and destination port number, along with the source and destination IP addresses in the IP header, uniquely identify each connection.</u>

The combination of an IP address and a port number is sometimes called an **endpoint** or **socket** in the TCP literature. The term "socket" appeared in [RFC0793] and was ultimately adopted as the name of the Berkeley-derived programming interface for network communications (called "Berkeley sockets"). It is a pair of sockets or endpoints (<u>the 4-tuple consisting of the client IP address, client port number, server IP address, and server port number) that uniquely identifies each TCP connection</u>. This fact will become important when we look at how a TCP server can communicate with multiple clients ([Chapter 13](ch13.md)).
