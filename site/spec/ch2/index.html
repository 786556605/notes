<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://notes.shichao.io/spec/ch2/">
        <link rel="shortcut icon" href="../../toki_32.png">
        

	<title>Chapter 2. Methodology - Shichao's Notes</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,400italic,500,600" rel="stylesheet">
        <link href="../../custom.css" rel="stylesheet">
        <link href="../../friendly.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">Shichao's Notes</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">APUE <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../apue/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch1/">Chapter 1. UNIX System Overview</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch2/">Chapter 2. UNIX Standardization and Implementations</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch3/">Chapter 3. File I/O</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch4/">Chapter 4. Files and Directories</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch5/">Chapter 5. Standard I/O Library</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch6/">Chapter 6. System Data Files and Information</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch7/">Chapter 7. Process Environment</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch8/">Chapter 8. Process Control</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch9/">Chapter 9. Process Relationships</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch10/">Chapter 10. Signals</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch11/">Chapter 11. Threads</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch12/">Chapter 12. Thread Control</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch13/">Chapter 13. Daemon Processes</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch14/">Chapter 14. Advanced I/O</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch15/">Chapter 15. Interprocess Communication</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch16/">Chapter 16. Network IPC: Sockets</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../apue/ch17/">Chapter 17. Advanced IPC</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">LKD <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../lkd/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch1/">Chapter 1. Introduction to the Linux Kernel</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch2/">Chapter 2. Getting Started with the Kernel</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch3/">Chapter 3. Process Management</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch4/">Chapter 4. Process Scheduling</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch5/">Chapter 5. System Calls</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch6/">Chapter 6. Kernel Data Structures</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch7/">Chapter 7. Interrupts and Interrupt Handlers</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch8/">Chapter 8. Bottom Halves and Deferring Work</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch9/">Chapter 9. An Introduction to Kernel Synchronization</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch10/">Chapter 10. Kernel Synchronization Methods</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch11/">Chapter 11. Timers and Time Management</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch12/">Chapter 12. Memory Management</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch13/">Chapter 13. The Virtual Filesystem</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch14/">Chapter 14. The Block I/O Layer</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch15/">Chapter 15. The Process Address Space</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../lkd/ch16/">Chapter 16. The Page Cache and Page Writeback</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">UNP <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../unp/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch2/">Chapter 2. The Transport Layer: TCP, UDP, and SCTP</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch3/">Chapter 3. Sockets Introduction</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch4/">Chapter 4. Elementary TCP Sockets</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch5/">Chapter 5. TCP Client/Server Example</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch6/">Chapter 6. I/O Multiplexing: The select and poll Functions</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../unp/ch7/">Chapter 7. Socket Options</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">TCPv1 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../tcpv1/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch2/">Chapter 2. The Internet Address Architecture</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch3/">Chapter 3. Link Layer</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch4/">Chapter 4. ARP: Address Resolution Protocol</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch5/">Chapter 5. The Internet Protocol (IP)</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch6/">Chapter 6. System Configuration: DHCP and Autoconfiguration</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch7/">Chapter 7. Firewalls and Network Address Translation (NAT)</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch8/">Chapter 8. ICMPv4 and ICMPv6: Internet Control Message Protocol</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch9/">Chapter 9. Broadcasting and Local Multicasting (IGMP and MLD)</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch10/">Chapter 10. User Datagram Protocol (UDP) and IP Fragmentation</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch11/">Chapter 11. Name Resolution and the Domain Name System (DNS)</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch12/">Chapter 12. TCP: The Transmission Control Protocol (Preliminaries)</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch13/">Chapter 13. TCP Connection Management</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch14/">Chapter 14. TCP Timeout and Retransmission</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch15/">Chapter 15. TCP Data Flow and Window Management</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch16/">Chapter 16. TCP Congestion Control</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch17/">Chapter 17. TCP Keepalive</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/ch18/">Chapter 18. Security: EAP, IPsec, TLS, DNSSEC, and DKIM</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../tcpv1/headers/">Headers</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
            
            
            
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">GOPL <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../gopl/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch1/">Chapter 1. Tutorial</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch2/">Chapter 2. Program Structure</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch3/">Chapter 3. Basic Data Types</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch4/">Chapter 4. Composite Types</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch5/">Chapter 5. Functions</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch6/">Chapter 6. Methods</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../gopl/ch7/">Chapter 7. Interfaces</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">TWGR <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../twgr/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../twgr/ch1/">Chapter 1. Bootstrapping your Ruby literacy</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../twgr/ch2/">Chapter 2. Objects, methods, and local variables</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
            
            
            
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">SPEC <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                    
                      
                        <li class="active">
                            <a href="./">Chapter 2. Methodology</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../ch3/">Chapter 3. Operating Systems</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">BD <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                      
                        <li >
                            <a href="../../bd/">Contents</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../bd/ch1/">Chapter 1. A new paradigm for Big Data</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../bd/ch2/">Chapter 2. Data model for Big Data</a>
                        </li>
                      
                    
                      
                        <li >
                            <a href="../../bd/ch3/">Chapter 3. Data model for Big Data: Illustration</a>
                        </li>
                      
                    
                    </ul>
                </li>
            
            
            
            
            
            
            
            
            
                <li >
                    <a href="../../roadmap/">Roadmap</a>
                </li>
            
            
            
            
            
            </ul>
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    
                        <a href="https://github.com/shichao-an/notes/blob/master/docs/spec/ch2.md">
                    
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#chapter-2-methodology">Chapter 2. Methodology</a></li>
        
    
        <li class="main "><a href="#terminology">Terminology</a></li>
        
    
        <li class="main "><a href="#models">Models</a></li>
        
            <li><a href="#system-under-test">System under Test</a></li>
        
            <li><a href="#queueing-system">Queueing System</a></li>
        
    
        <li class="main "><a href="#concepts">Concepts</a></li>
        
            <li><a href="#latency">Latency</a></li>
        
            <li><a href="#time-scales">Time Scales</a></li>
        
            <li><a href="#trade-offs">Trade-offs</a></li>
        
            <li><a href="#tuning-efforts">Tuning Efforts</a></li>
        
            <li><a href="#level-of-appropriateness">Level of Appropriateness</a></li>
        
            <li><a href="#point-in-time-recommendations">Point-in-Time Recommendations</a></li>
        
            <li><a href="#load-versus-architecture">Load versus Architecture</a></li>
        
            <li><a href="#scalability">Scalability</a></li>
        
            <li><a href="#known-unknowns">Known-Unknowns</a></li>
        
            <li><a href="#metrics">Metrics</a></li>
        
            <li><a href="#utilization">Utilization</a></li>
        
            <li><a href="#saturation">Saturation</a></li>
        
            <li><a href="#profiling">Profiling</a></li>
        
            <li><a href="#caching">Caching</a></li>
        
    
        <li class="main "><a href="#perspectives">Perspectives</a></li>
        
            <li><a href="#resource-analysis">Resource Analysis</a></li>
        
            <li><a href="#workload-analysis">Workload Analysis</a></li>
        
    
        <li class="main "><a href="#methodology">Methodology</a></li>
        
            <li><a href="#streetlight-anti-method">Streetlight Anti-Method</a></li>
        
            <li><a href="#random-change-anti-method">Random Change Anti-Method</a></li>
        
            <li><a href="#blame-someone-else-anti-method">Blame-Someone-Else Anti-Method</a></li>
        
            <li><a href="#ad-hoc-checklist-method">Ad Hoc Checklist Method</a></li>
        
            <li><a href="#problem-statement">Problem Statement</a></li>
        
            <li><a href="#scientific-method">Scientific Method</a></li>
        
            <li><a href="#diagnosis-cycle">Diagnosis Cycle</a></li>
        
            <li><a href="#tools-method">Tools Method</a></li>
        
            <li><a href="#the-use-method">The USE Method</a></li>
        
    
        <li class="main "><a href="#modeling">Modeling</a></li>
        
    
        <li class="main "><a href="#capacity-planning">Capacity Planning</a></li>
        
    
        <li class="main "><a href="#statistics">Statistics</a></li>
        
    
        <li class="main "><a href="#monitoring">Monitoring</a></li>
        
    
        <li class="main "><a href="#visualizations">Visualizations</a></li>
        
    
        <li class="main "><a href="#doubts-and-solutions">Doubts and Solutions</a></li>
        
            <li><a href="#verbatim">Verbatim</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">
              

<h3 id="chapter-2-methodology"><strong>Chapter 2. Methodology</strong></h3>
<p>Performance issues can arise from software, hardware, and any component along the data path. Methodologies help us approach complex systems by showing where to start and what steps to take to locate and analyze performance issues. [p15]</p>
<h3 id="terminology">Terminology</h3>
<p>The following are key terms for systems performance. Later chapters provide additional terms and describe some of these in different contexts.</p>
<ul>
<li><strong>IOPS</strong>: Input/output operations per second is a measure of the rate of data transfer operations.<ul>
<li>For disk I/O, IOPS refers to reads and writes per second.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Throughput"><strong>Throughput</strong></a>: the rate of work performed. Especially in communications, the term is used to refer to the <a href="https://en.wikipedia.org/wiki/Data_rate_units">data rate</a> (bytes per second or bits per second).<ul>
<li>In some contexts (e.g., databases), throughput can refer to the operation rate (operations per second or transactions per second).</li>
</ul>
</li>
<li><strong>Response time</strong>: the time for an operation to complete. This includes any time spent waiting and time spent being serviced (service time), including the time to transfer the result.</li>
<li><strong>Latency</strong>: Latency is a measure of time an operation spends waiting to be serviced.<ul>
<li>In some contexts, it can refer to the entire time for an operation, equivalent to response time (<a href="#Concepts">Section 2.3</a>).</li>
</ul>
</li>
<li><strong>Utilization</strong>:<ul>
<li>For resources that service requests, utilization is a measure of how busy a resource is, based on how much time in a given interval it was actively performing work.</li>
<li>For resources that provide storage, utilization may refer to the capacity that is consumed (e.g., memory utilization).</li>
</ul>
</li>
<li><strong>Saturation</strong>: the degree to which a resource has queued work it cannot service.</li>
<li><strong>Bottleneck</strong>: In system performance, a bottleneck is a resource that limits the performance of the system. Identifying and removing systemic bottlenecks is a key activity of systems performance.</li>
<li><strong>Workload</strong>: The input to the system or the load applied is the workload. For a database, the workload consists of the database queries and commands sent by the clients.</li>
<li><strong>Cache</strong>: a fast storage area that can duplicate or buffer a limited amount of data, to avoid communicating directly with a slower tier of storage, thereby improving performance. For economic reasons, a cache is smaller than the slower tier.</li>
</ul>
<h3 id="models">Models</h3>
<h4 id="system-under-test">System under Test</h4>
<p>The performance of a <a href="https://en.wikipedia.org/wiki/System_under_test">system under test</a> (SUT) is shown below:</p>
<p><a href="../figure_2.1.png" title="Figure 2.1 System under test"><img alt="Figure 2.1 System under test" src="../figure_2.1.png" /></a></p>
<p><strong>Perturbations</strong> (interference) can affect results, including those caused by:</p>
<ul>
<li>Scheduled system activity,</li>
<li>Other users of the system,</li>
<li>Oher workloads.</li>
</ul>
<p>The origin of the perturbations may not be clear and determining it can be particularly difficult in some cloud environments, where other activity (by guest tenants) on the physical host system is not observable from within a guest SUT.</p>
<p>Another difficulty is that modern environments may be composed of several networked components needed to service the input workload, including load balancers, web servers, database servers, application servers, and storage systems. The mere act of mapping the environment may help to reveal previously overlooked sources of perturbations. The environment may also be modeled as a network of queueing systems, for analytical study.</p>
<h4 id="queueing-system">Queueing System</h4>
<p>Some components and resources can be modeled as a queueing system. The following figure shows a simple queueing system.</p>
<p><a href="../figure_2.2.png" title="Figure 2.2 Simple queueing model"><img alt="Figure 2.2 Simple queueing model" src="../figure_2.2.png" /></a></p>
<h3 id="concepts">Concepts</h3>
<h4 id="latency">Latency</h4>
<p>The <strong>latency</strong> is the time spent waiting before an operation is performed.  The following figure, as an example of latency, shows a network transfer (e.g. HTTP GET request):</p>
<p><a href="../figure_2.3.png" title="Figure 2.3 Network connection latency"><img alt="Figure 2.3 Network connection latency" src="../figure_2.3.png" /></a></p>
<p>In this example, the operation is a network service request to transfer data. Before this operation can take place, the system must wait for a network connection to be established, which is latency for this operation. The response time spans this latency and the operation time.</p>
<p>Depending on the target, the latency can be measured differently. For example, the load time for a website may be composed of three different times:</p>
<ul>
<li>DNS latency, which refers to the entire DNS operation.</li>
<li>TCP connection latency, which refers to the initialization only (TCP handshake).</li>
<li>TCP data transfer time.</li>
</ul>
<p>At a higher level, the response time may be termed latency. [p19]</p>
<p>Time orders of magnitude and their abbreviations are listed in the following table:</p>
<table>
<thead>
<tr>
<th>Unit</th>
<th>Abbreviation</th>
<th>Fraction of 1 s</th>
</tr>
</thead>
<tbody>
<tr>
<td>Minute</td>
<td>m</td>
<td>60</td>
</tr>
<tr>
<td>Second</td>
<td>s</td>
<td>1</td>
</tr>
<tr>
<td>Millisecond</td>
<td>ms</td>
<td>0.001 or 1/1000 or 1 x 10<sup>-3</sup></td>
</tr>
<tr>
<td>Microsecond</td>
<td>μs</td>
<td>0.000001 or 1/1000000 or 1 x 10<sup>-6</sup></td>
</tr>
<tr>
<td>Nanosecond</td>
<td>ns</td>
<td>0.000000001 or 1/1000000000 or 1 x 10<sup>-9</sup></td>
</tr>
<tr>
<td>Picosecond</td>
<td>ps</td>
<td>0.000000000001 or 1/1000000000000 or 1 x 10<sup>-12</sup></td>
</tr>
</tbody>
</table>
<p>When possible, other metric types can be converted to latency or time so that they can be compared. For example:</p>
<ul>
<li>Choosing the better performance between 100 network I/O or 50 disk I/O be a complicated choice, involving many factors: network hops, rate of network drops and retransmits, I/O size, random or sequential I/O, disk types, etc..</li>
<li>Comparing 100 ms of total network I/O and 50 ms of total disk I/O is easier.</li>
</ul>
<h4 id="time-scales">Time Scales</h4>
<p>System components operate over vastly different time scales (orders of magnitude).</p>
<p>The following table is an example Time Scale of System Latencies (3.3 GHz processor):</p>
<table>
<thead>
<tr>
<th>Event</th>
<th>Latency</th>
<th>Scaled</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 CPU cycle</td>
<td>0.3 ns</td>
<td>1 s</td>
</tr>
<tr>
<td>Level 1 cache access</td>
<td>0.9 ns</td>
<td>3 s</td>
</tr>
<tr>
<td>Level 2 cache access</td>
<td>2.8 ns</td>
<td>9 s</td>
</tr>
<tr>
<td>Level 3 cache access</td>
<td>12.9 ns</td>
<td>43 s</td>
</tr>
<tr>
<td>Main memory access (DRAM, from CPU)</td>
<td>120 ns</td>
<td>6 min</td>
</tr>
<tr>
<td>Solid-state disk I/O (flash memory)</td>
<td>50–150 μs</td>
<td>2–6 days</td>
</tr>
<tr>
<td>Rotational disk I/O</td>
<td>1–10 ms</td>
<td>1–12 months</td>
</tr>
<tr>
<td>Internet: San Francisco to New York</td>
<td>40 ms</td>
<td>4 years</td>
</tr>
<tr>
<td>Internet: San Francisco to United Kingdom</td>
<td>81 ms</td>
<td>8 years</td>
</tr>
<tr>
<td>Internet: San Francisco to Australia</td>
<td>183 ms</td>
<td>19 years</td>
</tr>
<tr>
<td>TCP packet retransmit</td>
<td>1–3 s</td>
<td>105–317 years</td>
</tr>
<tr>
<td>OS virtualization system reboot</td>
<td>4 s</td>
<td>423 years</td>
</tr>
<tr>
<td>SCSI command time-out</td>
<td>30 s</td>
<td>3 millennia</td>
</tr>
<tr>
<td>Hardware (HW) virtualization system reboot</td>
<td>40 s</td>
<td>4 millennia</td>
</tr>
<tr>
<td>Physical system reboot</td>
<td>5 m</td>
<td>32 millennia</td>
</tr>
</tbody>
</table>
<h4 id="trade-offs">Trade-offs</h4>
<p>Be aware of some common performance trade-offs. The figure below shows the good/fast/cheap "pick two" trade-off on the left alongside the terminology adjusted for IT projects on the right.</p>
<p><a href="../figure_2.4.png" title="Figure 2.4 Trade-offs: pick two"><img alt="Figure 2.4 Trade-offs: pick two" src="../figure_2.4.png" /></a></p>
<p>A common trade-off in performance tuning is that between CPU and memory:</p>
<ul>
<li>Memory can be used to cache results, reducing CPU usage.</li>
<li>CPU may be spent to compress data to reduce memory usage. (On modern systems with an abundance of CPU)</li>
</ul>
<p>[p21]</p>
<p>Tunable parameters often come with trade-offs. For examples:</p>
<ul>
<li><strong>File system record size</strong> (or block size):<ul>
<li>Small record sizes, close to the application I/O size, will perform better for random I/O workloads and make more efficient use of the file system cache while the application is running.</li>
<li>Large record sizes will improve streaming workloads, including file system backups.</li>
</ul>
</li>
<li><strong>Network buffer size</strong>:<ul>
<li>Small buffer sizes will reduce the memory overhead per connection, helping the system scale.</li>
<li>Large sizes will improve network throughput.</li>
</ul>
</li>
</ul>
<h4 id="tuning-efforts">Tuning Efforts</h4>
<p><u>Performance tuning is most effective when done closest to where the work is performed (e.g. within application itself)).</u> The following table shows an example of software stack, with tuning possibilities.</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Tuning Targets</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>database queries performed</td>
</tr>
<tr>
<td>Database</td>
<td>database table layout, indexes, buffering</td>
</tr>
<tr>
<td>System calls</td>
<td>memory-mapped or read/write, sync or async I/O flags</td>
</tr>
<tr>
<td>File system</td>
<td>record size, cache size, file system tunables</td>
</tr>
<tr>
<td>Storage</td>
<td>RAID level, number and type of disks, storage tunables</td>
</tr>
</tbody>
</table>
<h5 id="application-level"><strong>Application Level</strong> *</h5>
<p>Tuning at the application level may improve performance significantly due to the following reasons:</p>
<ol>
<li>It may be possible to eliminate or reduce database queries and improve performance by a large factor (e.g., 20x).<ul>
<li>Tuning down to the storage device level may eliminate or improve storage I/O, but tuning efforts have already been made executing higher-level OS stack code, so this may improve resulting application performance by only percentages (e.g., 20%).</li>
</ul>
</li>
<li>Since many of today’s environments target rapid deployment for features and functionality, application development and testing tend to focus on correctness, leaving little or no time for performance measurement or optimization before production deployment. These activities are conducted later, when performance becomes a problem.</li>
</ol>
<p>The application isn’t necessarily the most effective level from which to base observation. Slow queries may be best understood from their time spent on-CPU, or from the file system and disk I/O that they perform. These are observable from operating system tools.</p>
<p>In many environments (especially cloud computing), the application level is under constant development, pushing software changes into production weekly or daily. Large performance improvment (including fixes for regressions) are frequently found as the application code changes. In these environments, tuning for the operating system and observability from the operating system can be easy to overlook. Remember that operating system performance analysis can also identify application-level issues, not just OS-level issues, in some cases more easily than from the application alone.</p>
<h4 id="level-of-appropriateness">Level of Appropriateness</h4>
<p>Different organizations and environments have different requirements for performance [p22]. This doesn’t necessarily mean that some organizations are doing it right and some wrong. It depends on the <a href="https://en.wikipedia.org/wiki/Return_on_investment">return on investment</a> (ROI) for performance expertise:</p>
<ul>
<li>Organizations with large data centers or cloud environments may need a team of performance engineers who analyze everything, including kernel internals and CPU performance counters, and frequently use dynamic tracing. They may also formally model performance and develop accurate predictions for future growth.</li>
<li>Small start-ups may have time only for superficial checks, trusting third-party monitoring solutions to check their performance and provide alerts.</li>
</ul>
<h4 id="point-in-time-recommendations">Point-in-Time Recommendations</h4>
<p>The performance characteristics of environments change over time, due to the addition of more users, newer hardware, and updated software or firmware.</p>
<p>[p23]</p>
<p>Performance recommendations, especially the values of tunable parameters, are valid only at a specific <em>point in time</em>. What may have been the best advice from a performance expert one week may become invalid a week later after a software or hardware upgrade, or after adding more users.</p>
<h4 id="load-versus-architecture">Load versus Architecture</h4>
<p>An application can perform badly due to an issue with the software configuration and hardware on which it is running: its architecture. However, an application can also perform badly simply due to too much load applied, resulting in queueing and long latencies. Load and architecture are pictured in the figure below:</p>
<p><a href="../figure_2.5.png" title="Figure 2.5 Load versus architecture"><img alt="Figure 2.5 Load versus architecture" src="../figure_2.5.png" /></a></p>
<p>If analysis of the architecture shows queueing of work but no problems with how the work is performed, the issue may be one of too much load applied. In a cloud computing environment, this is the point where more nodes can be introduced to handle the work.</p>
<h5 id="single-threaded-and-multithreaded-application"><strong>Single-threaded and multithreaded application</strong> *</h5>
<p>For example,</p>
<ul>
<li>An <strong>issue of architecture</strong> may be a single-threaded application that is busy on-CPU, with requests queueing while other CPUs are available and idle. In this case, performance is limited by the application’s single-threaded architecture.</li>
<li>An <strong>issue of load</strong> may be a multithreaded application that is busy on all available CPUs, with requests still queueing. In this case, performance is limited by the available CPU capacity, or put differently, by more load than the CPUs can handle.</li>
</ul>
<h4 id="scalability">Scalability</h4>
<p>The performance of the system under increasing load is its <strong>scalability</strong>. The following figure shows a typical throughput profile as a system’s load increases:</p>
<p><a href="../figure_2.6.png" title="Figure 2.6 Throughput versus load"><img alt="Figure 2.6 Throughput versus load" src="../figure_2.6.png" /></a></p>
<p>For some period, linear scalability is observed. A point is then reached, marked with a dotted line, where contention for a resource begins to affect performance. This point can be described as a <em>knee point</em>, as it is the boundary between two pro files. Beyond this point, the throughput profile departs from linear scalability, as contention for the resource increases. Eventually the overheads for increased contention and coherency cause less work to be completed and throughput to decrease.</p>
<p>This point may occur when a component reaches 100% utilization: the <em>saturation point</em>. It may also occur when a component approaches 100% utilization, and queueing begins to be frequent and significant. This point may occur when a component reaches 100% utilization: the saturation
point. It may also occur when a component approaches 100% utilization, and
queueing begins to be frequent and significant.</p>
<p>An example system that may exhibit this profile is an application that performs heavy compute, with more load added as threads. As the CPUs approach 100% utilization, performance begins to degrade as CPU scheduler latency increases. After peak performance, at 100% utilization, throughput begins to decrease as more threads are added, causing more context switches, which consume CPU resources and cause less actual work to be completed.</p>
<p>The same curve can be seen if you replace "load" on the <em>x</em> axis with a resource such as CPU cores (detailed in <a href="#modeling">Modeling</a>)</p>
<p>The degradation of performance for nonlinear scalability, in terms of average response time or latency, is graphed in the following figure:</p>
<p><a href="../figure_2.7.png" title="Figure 2.7 Performance degradation"><img alt="Figure 2.7 Performance degradation" src="../figure_2.7.png" /></a></p>
<ul>
<li>The "fast" degradation profile may occur for memory load, when the system begins to page (or swap) to supplement main memory.</li>
<li>The "slow" degradation profile may occur for CPU load.</li>
<li>Another "fast" profile example is disk I/O. As load (and the resulting disk utilization) increases, I/O becomes more likely to queue behind other I/O. An idle rotational disk may serve I/O with a response time of about 1 ms, but when load increases, this can approach 10 ms.</li>
</ul>
<p><u>Linear scalability of response time could occur if the application begins to return errors when resources are unavailable, instead of queueing work. For example, a web server may return 503 "Service Unavailable" instead of adding requests to a queue, so that those requests that are served can be performed with a consistent response time.</u></p>
<h4 id="known-unknowns">Known-Unknowns</h4>
<p>The following notions are important:</p>
<ul>
<li><strong>Known-knowns</strong>: These are things you know. You know you should be checking a performance metric, and you know its current value. For example, you know you should be checking CPU utilization, and you also know that the value is 10% on average.</li>
<li><strong>Known-unknowns</strong>: These are things you know that you do not know. You know you can check a metric or the existence of a subsystem, but you haven’t yet observed it. For example, you know you could be checking what is making the CPUs busy by the use of profiling but have yet to do so.</li>
<li><strong>Unknown-unknowns</strong>: These are things you do not know you do not know.  For example, you may not know that device interrupts can become heavy CPU consumers, so you are not checking them.</li>
</ul>
<p>Performance is a field where "the more you know, the more you don’t know". It’s the same principle: the more you learn about systems, the more unknownunknowns you become aware of, which are then known-unknowns that you can check on.</p>
<h4 id="metrics">Metrics</h4>
<p>Performance metrics are statistics generated by the system, applications, or additional tools that measure activity of interest. They are studied for performance analysis and monitoring, either numerically at the command line or graphically using visualizations.</p>
<p>Common types of systems performance metrics include:</p>
<ul>
<li><strong>IOPS</strong>: I/O operations per second</li>
<li><strong>Throughput</strong>: either operations or volume per second, depending on its context:<ul>
<li>Database throughput is usually a measure of queries or requests (operations) per second.</li>
<li>Network throughput is a measure of bits or bytes (volume) per second.</li>
</ul>
</li>
<li><strong>Utilization</strong></li>
<li><strong>Latency</strong></li>
</ul>
<h5 id="overhead"><strong>Overhead</strong></h5>
<p>Since CPU cycles must be spent to gather and store the metrics. This causes overhead, which can negatively affect the performance of the target of measurement. This is called the <a href="https://en.wikipedia.org/wiki/Observer_effect_(information_technology)"><strong>observer effect</strong></a>.</p>
<h5 id="issues"><strong>Issues</strong></h5>
<p>The temptation is to assume that the software vendor has provided metrics that are well chosen, are bug-free, and provide complete visibility. In reality, metrics can be confusing, complicated, unreliable, inaccurate, and even plain wrong (due to bugs). Sometimes a metric was correct on one software version but did not get updated to reflect the addition of new code and code paths.</p>
<h4 id="utilization">Utilization</h4>
<p>The term <em>utilization</em> is often used for operating systems to describe device usage, such as for the CPU and disk devices. Utilization can be time-based or capacitybased.</p>
<h5 id="time-based"><strong>Time-Based</strong></h5>
<p>Time-based utilization is the average amount of time the server or resource was busy, as defined in <a href="https://en.wikipedia.org/wiki/Queueing_theory">queueing theory</a>, along with the ratio:</p>
<blockquote>
<p><em>U</em> = <em>B</em>/<em>T</em></p>
</blockquote>
<p>where:</p>
<ul>
<li><em>U</em> = utilization</li>
<li><em>B</em> = total time the system was busy during <em>T</em>, the observation period</li>
</ul>
<p>The "utilization" is also available from operating system performance tools. The disk monitoring tool <a href="http://man7.org/linux/man-pages/man1/iostat.1.html"><code>iostat(1)</code></a> calls this metric <em>%b</em> for percent busy, a term that better conveys the underlying metric: <em>B</em>/<em>T</em>.</p>
<p>This utilization metric means how busy a component is: when a component approaches 100% utilization, performance can seriously degrade when there is contention for the resource. Other metrics can be checked to confirm and to see if the component has therefore become a system bottleneck.</p>
<p>Some components can service multiple operations in parallel. Performance may not degrade much at 100% utilization, as they can accept more work. [p28]</p>
<ul>
<li>A disk that is 100% busy may also be able to accept and process more work, for example, by buffering writes in the on-disk cache to be completed later.</li>
<li>Storage arrays frequently run at 100% utilization because some disk is busy 100% of the time, but the array has plenty of idle disks and can accept much more work.</li>
</ul>
<h5 id="capacity-based"><strong>Capacity-Based</strong></h5>
<p>The other definition of utilization in the context of capacity planning is:</p>
<blockquote>
<p>A system or component (such as a disk drive) is able to deliver a certain amount of throughput. At any level of performance, the system or component is working at some proportion of its capacity. That proportion is called the utilization.</p>
</blockquote>
<p>This defines utilization in terms of capacity instead of time. It implies that a disk at 100% utilization cannot accept any more work. With the time-based definition, 100% utilization only means it is busy 100% of the time. Therefore, <u>100% busy does not mean 100% capacity.</u></p>
<h5 id="time-based-vs-capacity-based"><strong>Time-Based vs. Capacity-Based</strong> *</h5>
<p>Use elevator as an example:</p>
<ul>
<li>Time-Based Utilization: the elevator may be considered utilized when it is moving between floors, and not utilized when it is idle waiting. However, the elevator may be able to accept more passengers even when it is busy 100% of the time responding to calls.</li>
<li>Capacity-Based Utilization: 100% capacity may mean the elevator is at its maximum payload capacity and cannot accept more passengers.</li>
</ul>
<p>In an ideal world, we would be able to measure both types of utilization for a device, which usually isn't possible. [p29]</p>
<p><u>In this book, <em>utilization</em> usually refers to the time-based version.</u> The capacity version is used for some volume-based metrics, such as memory usage.</p>
<h5 id="non-idle-time"><strong>Non-Idle Time</strong></h5>
<p><strong>Non-idle time</strong> is a more accurate term to define utilization, but not yet in common usage. [p29]</p>
<h4 id="saturation">Saturation</h4>
<p><strong>Saturation</strong> is the degree to which more work is requested of a resource than it can process. Saturation begins to occur at 100% utilization (capacity-based), as extra work cannot be processed and begins to queue. This is pictured in the following figure:</p>
<p><a href="../figure_2.8.png" title="Figure 2.8 Utilization versus saturation"><img alt="Figure 2.8 Utilization versus saturation" src="../figure_2.8.png" /></a></p>
<p>Any degree of saturation is a performance issue, as time is spent waiting (latency). For time-based utilization (percent busy), saturation may not begin at the 100% utilization mark, depending on the degree to which the resource can operate on work in parallel. [p30]</p>
<h4 id="profiling">Profiling</h4>
<p><strong>Profiling</strong> is typically performed by <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling</a> the state of the system at timed intervals, and then studying the set of samples.</p>
<p>Unlike the previous metrics covered, including IOPS and throughput, the use of sampling provides a <em>coarse</em> view of the target’s activity, depending on the rate of sampling.</p>
<p>For example, CPU usage can be understood in reasonable detail by sampling the CPU program counter or stack backtrace at frequent intervals to gather statistics on the code paths that are consuming CPU resources, which is detailed in <a href="ch6.md">Chapter 6</a>.</p>
<h4 id="caching">Caching</h4>
<p>Frequently used to improve performance, a cache stores results from a slower storage tier in a faster storage tier for reference. An example is caching disk blocks in main memory (RAM).</p>
<ul>
<li>Multiple tiers of caches may be used. CPUs commonly employ multiple hardware caches for main memory (Levels 1, 2, and 3), beginning with a very fast but small cache (Level 1) and increasing in both storage size and access latency. This is an economic trade-off between density and latency:level; and sizes are chosen for the best performance for the on-chip space available.</li>
<li>There are many other caches present in a system, many of them implemented in software using main memory for storage.</li>
</ul>
<p>Caching is detailed in <a href="../ch3/#caching">Section 3.2.11</a>.</p>
<h5 id="cache-metrics"><strong>Cache metrics</strong> *</h5>
<p><strong>Hit ratio</strong> is a metric of cache performance. It represents the number of times the needed data was found in the cache (hits) versus the number of times it was not (misses). The higher, the better, as a higher ratio reflects more data successfully accessed from faster media. The following figure shows the expected performance improvement for increasing cache hit ratios.</p>
<p><a href="../figure_2.9.png" title="Figure 2.9 Cache hit ratio and performance"><img alt="Figure 2.9 Cache hit ratio and performance" src="../figure_2.9.png" /></a></p>
<p>This is a nonlinear profile because of the difference in speed between cache hits and misses (the two storage tiers). The performance difference between 98% and 99% is much greater than that between 10% and 11%. The greater the difference, the steeper the slope becomes.</p>
<p><strong>Miss rate</strong> is another metric, in terms of misses per second. This is proportional (linear) to the performance penalty of each miss.</p>
<p>The total runtime for each workload can be calculated as:</p>
<blockquote>
<p>runtime = (hit rate x hit latency) + (miss rate x miss latency)</p>
</blockquote>
<p>This calculation uses the average hit and miss latencies and assumes the work is serialized.</p>
<h5 id="algorithms"><strong>Algorithms</strong></h5>
<p>Cache management algorithms and policies determine what to store in the limited space available for a cache:</p>
<ul>
<li><strong>Most recently used</strong> (MRU) refers to a cache <strong>retention policy</strong>, which decides what to favor keeping in the cache: the objects that have been used most recently.</li>
<li><strong>Least recently used</strong> (LRU) can refer to an equivalent cache <strong>eviction policy</strong>, deciding what objects to remove from the cache when more space is needed.</li>
<li><strong>Most frequently used</strong> (MFU)</li>
<li><strong>Least frequently used</strong> (LFU)</li>
<li><strong>Not frequently used</strong> (NFU): an inexpensive but less thorough version of LRU.</li>
</ul>
<h5 id="hot-cold-and-warm-caches"><strong>Hot, Cold, and Warm Caches</strong></h5>
<p>The following words describe the state of the cache:</p>
<ul>
<li>Cold: A <strong>cold cache</strong> is empty, or populated with unwanted data. The hit ratio for a cold cache is zero (or near zero as it begins to warm up).</li>
<li>Hot: A <strong>hot cache</strong> is populated with commonly requested data and has a high hit ratio, for example, over 99%.</li>
<li>Warm: A <strong>warm cache</strong> is one that is populated with useful data but doesn’t have a high enough hit ratio to be considered hot.</li>
<li>Warmth: Cache warmth describes how hot or cold a cache is. An activity that improves cache warmth is one that aims to improve the cache hit ratio.</li>
</ul>
<p>When caches are first initialized, they begin cold and then warm up over time.  When the cache is large or the next-level storage is slow (or both), the cache can take a long time to become populated and warm.</p>
<p>[p32]</p>
<h3 id="perspectives">Perspectives</h3>
<p>There are two common perspectives for performance analysis: <strong>workload analysis</strong> and <strong>resource analysis</strong>, which can be thought of as either top-down or bottom-up analysis of the operating system software stack, as show in the figure below:</p>
<p><a href="../figure_2.10.png" title="Figure 2.10 Analysis perspectives"><img alt="Figure 2.10 Analysis perspectives" src="../figure_2.10.png" /></a></p>
<h4 id="resource-analysis">Resource Analysis</h4>
<p>Resource analysis begins with analysis of the system resources: CPUs, memory, disks, network interfaces, busses, and interconnects. It is most likely performed by system administrators, who are responsible for the physical environment resources.</p>
<h5 id="activities"><strong>Activities</strong> *</h5>
<ul>
<li><strong>Performance issue investigations:</strong> to see if a particular type of resource is responsible</li>
<li><strong>Capacity planning</strong>: for information to help size new systems, and to see when existing system resources may become exhausted</li>
</ul>
<h5 id="metrics-with-utilization-as-a-focus"><strong>Metrics with utilization as a focus</strong> *</h5>
<p>Resource analysis focuses on utilization to identify when resources are at or approaching their limit.</p>
<ul>
<li>Some resource types, such as CPUs, have utilization metrics readily available.</li>
<li>Utilization for other resources can be estimated based on available metrics, for example, estimating network interface utilization by comparing the send and receive megabits per second (throughput) with the known maximum bandwidth.</li>
</ul>
<p><a href="#metrics">Metrics</a> best suited for resource analysis include:</p>
<ul>
<li>IOPS</li>
<li>Throughput</li>
<li><a href="#utilization">Utilization</a></li>
<li><a href="#saturation">Saturation</a></li>
</ul>
<p>These metrics measure the following:</p>
<ul>
<li>What the resource is being asked to do</li>
<li>How utilized or saturated
it is for a given load. Ot</li>
</ul>
<p>Other types of metrics, including latency, are also of use to see how well the resource is responding for the given workload.</p>
<h5 id="documentation-on-stat-tools"><strong>Documentation on "stat" tools</strong> *</h5>
<p>Resource analysis is a common approach to performance analysis, in part because of the widely available documentation on the topic. Such documentation focuses on the operating system "stat" tools: <a href="http://man7.org/linux/man-pages/man8/vmstat.8.html"><code>vmstat(1)</code></a>, <a href="http://man7.org/linux/man-pages/man1/iostat.1.html"><code>iostat(1)</code></a>, <a href="http://man7.org/linux/man-pages/man1/mpstat.1.html"><code>mpstat(1)</code></a>. Resource analysis is a perspective, but not the only perspective.</p>
<h4 id="workload-analysis">Workload Analysis</h4>
<p>Workload analysis, as seen in the figure below, examines the performance of the applications, including the workload applied and how the application is responding. It is most commonly used by application developers and support staff, who are responsible for the application software and configuration.</p>
<p><a href="../figure_2.11.png" title="Figure 2.11 Workload analysis"><img alt="Figure 2.11 Workload analysis" src="../figure_2.11.png" /></a></p>
<h5 id="targets-for-workload-analysis"><strong>Targets for workload analysis</strong> *</h5>
<ul>
<li><strong>Requests</strong>: the workload applied</li>
<li><strong>Latency</strong>: the response time of the application</li>
<li><strong>Completion</strong>: looking for errors</li>
</ul>
<p>Studying workload requests involves checking and summarizing their attributes: the process of <em>workload characterization</em> (detailed in <a href="#methodology">Section 2.5</a>). For databases, these attributes may include the client host, database name, tables, and query string. This data may help identify unnecessary work or unbalanced work. Examining these attributes may identify ways to reduce or eliminate the work applied. (The fastest query is the one you don’t do at all.)</p>
<p>Latency (response time) is the most important metric for expressing application performance. For instance: for a MySQL database, it’s query latency; for Apache, it’s HTTP request latency. In these contexts, the term <em>latency</em> is used to mean the same as response time (<a href="#latency">Section 2.3.1</a>).</p>
<h5 id="identifying-issues"><strong>Identifying issues</strong></h5>
<p>The tasks of workload analysis are identifying and confirming issues. Latency, for example, can be done by:</p>
<ol>
<li>Looking for latency beyond an acceptable threshold,</li>
<li>Finding the source of the latency (drill-down analysis),</li>
<li>Confirming that the latency is improved after applying a fix.</li>
</ol>
<p>Note that the starting point is the application. To investigate latency usually involves drilling down deeper into the application, libraries, and the operating system (kernel).</p>
<p>System issues may be identified by studying characteristics related to the completion of an event, including its error status. While a request may complete quickly, it may do so with an error status that causes the request to be retried, accumulating latency.</p>
<h5 id="metrics-for-workload-analysis"><strong>Metrics for workload analysis</strong> *</h5>
<ul>
<li>Throughput (transactions per second)</li>
<li>Latency</li>
</ul>
<p>These measure the rate of requests and the resulting performance.</p>
<h3 id="methodology">Methodology</h3>
<p>This section describes methodologies and procedures for system performance analysis and tuning, and introduces some new methods, particularly the <a href="#the-use-method">USE method</a>. Some <a href="https://en.wiktionary.org/wiki/antimethodology">anti-methodologies</a> have also been included.</p>
<p>These methodologies have been categorized as different types in the following table:</p>
<table>
<thead>
<tr>
<th>Methodology</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Streetlight anti-method</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Random change anti-method</td>
<td>experimental analysis</td>
</tr>
<tr>
<td>Blame-someone-else anti-method</td>
<td>hypothetical analysis</td>
</tr>
<tr>
<td>Ad hoc checklist method</td>
<td>observational and experimental analysis</td>
</tr>
<tr>
<td>Problem statement</td>
<td>information gathering</td>
</tr>
<tr>
<td>Scientific method</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Diagnosis cycle</td>
<td>analysis life cycle</td>
</tr>
<tr>
<td>Tools method</td>
<td>observational analysis</td>
</tr>
<tr>
<td>USE method</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Workload characterization</td>
<td>observational analysis, capacity planning</td>
</tr>
<tr>
<td>Drill-down analysis</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Latency analysis</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Method R</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Event tracing</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Baseline statistics</td>
<td>observational analysis</td>
</tr>
<tr>
<td>Performance monitoring</td>
<td>observational analysis, capacity planning</td>
</tr>
<tr>
<td>Queueing theory</td>
<td>statistical analysis, capacity planning</td>
</tr>
<tr>
<td>Static performance tuning</td>
<td>observational analysis, capacity planning</td>
</tr>
<tr>
<td>Cache tuning</td>
<td>observational analysis, tuning</td>
</tr>
<tr>
<td>Micro-benchmarking</td>
<td>experimental analysis</td>
</tr>
<tr>
<td>Capacity planning</td>
<td>capacity planning, tuning</td>
</tr>
</tbody>
</table>
<p>The following sections begin with commonly used but weaker methodologies for comparison, including the anti-methodologies. For the analysis of performance issues, the first methodology you should attempt is the <a href="#problem-statement">problem statement</a> method, before moving on to others.</p>
<h4 id="streetlight-anti-method">Streetlight Anti-Method</h4>
<p>This method is actually the <em>absence</em> of a deliberate methodology. The user analyzes performance by choosing observability tools that are familiar, found on the Internet, or at random to see if anything obvious shows up. This approach is hit or miss and can overlook many types of issues.</p>
<p>Tuning performance may be attempted in a similar <a href="https://en.wikipedia.org/wiki/Trial_and_error">trial-and-error</a> fashion, setting whatever tunable parameters are known and familiar to different values to see if that helps.</p>
<p>Even when this method reveals an issue, it can be slow as tools or tunings unrelated to the issue are found and tried, just because they’re familiar. This methodology is therefore named after an observational bias called the <a href="https://en.wikipedia.org/wiki/Streetlight_effect"><strong>streetlight effect</strong></a>, illustrated by this parable:</p>
<blockquote>
<p>One night a police officer sees a drunk searching the ground beneath a streetlight and asks what he is looking for. The drunk says he has lost his keys. The police officer can’t find them either and asks: "Are you sure you lost them here, under the streetlight?" The drunk replies: "No, but this is where the light is best."</p>
</blockquote>
<p>The performance equivalent would be looking at <a href="http://man7.org/linux/man-pages/man1/top.1.html"><code>top(1)</code></a>, not because it makes sense, but because the user doesn’t know how to read other tools. This methodology does find may be <em>an issue</em> but not <em>the issue</em>. Other methodologies quantify findings, so that <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positives</a> can be ruled out more quickly.</p>
<h4 id="random-change-anti-method">Random Change Anti-Method</h4>
<p>This is an experimental anti-methodology. The user randomly guesses where the problem may be and then changes things until it goes away. To determine whether performance has improved as a result of each change, a metric is studied, such as:</p>
<ul>
<li>Application runtime</li>
<li>Operation time</li>
<li>Latency</li>
<li>Operation rate (operations per second)</li>
<li>Throughput (bytes per second)</li>
</ul>
<p>This approach is as follows:</p>
<ol>
<li>Pick a random item to change (e.g., a tunable parameter).</li>
<li>Change it in one direction.</li>
<li>Measure performance.</li>
<li>Change it in the other direction.</li>
<li>Measure performance.</li>
<li>Check whether the results in step 3 or step 5 better than the baseline. If so, keep the change and go back to step 1.</li>
</ol>
<h5 id="cons-of-the-random-change-anti-method"><strong>Cons of the Random Change Anti-Method</strong> *</h5>
<p>Although this process may eventually unearth tuning that works for the tested workload, it has the following disadvantages:</p>
<ol>
<li>It is very time-consuming and can also leave behind tuning that doesn’t make sense in the long term.<ul>
<li>For example, an application change may improve performance because it works around a database or operating system bug, which is later fixed. But the application will still have that tuning that no longer makes sense, and that no one understood properly in the first place.</li>
</ul>
</li>
<li>A change that isn’t properly understood causes a worse problem during peak production load and a need to back out the change during this time.</li>
</ol>
<h4 id="blame-someone-else-anti-method">Blame-Someone-Else Anti-Method</h4>
<p>This anti-methodology follows these steps:</p>
<ol>
<li>Find a system or environment component for which you are not responsible.</li>
<li>Hypothesize that the issue is with that component.</li>
<li>Redirect the issue to the team responsible for that component.</li>
<li>When proven wrong, go back to step 1.</li>
</ol>
<blockquote>
<p>Maybe it’s the network. Can you check with the network team if they have had dropped packets or something?</p>
</blockquote>
<p>[p38]</p>
<p>This anti-methodology can be identified by a lack of data leading to the hypothesis. To avoid becoming a victim of blame-someone-else, ask the accuser for screen shots showing which tools were run and how the output was interpreted. You can take these screen shots and interpretations to someone else for a second opinion.</p>
<h4 id="ad-hoc-checklist-method">Ad Hoc Checklist Method</h4>
<p>The ad hoc checklists used by support professionals are built from recent experience and issues for that system type. A typical scenario involves the deployment of a new server or application in production and a support professional checking for common issues when the system is under real load.</p>
<p>The following is an example checklist entry:</p>
<blockquote>
<p>Run <code>iostat –x 1</code> and check the await column. If this is consistently over 10 (ms) during load, then the disks are either slow or overloaded.</p>
</blockquote>
<h5 id="cons-of-ad-hoc-checklists"><strong>Cons of ad hoc checklists</strong> *</h5>
<p>While these checklists can provide the most value in the shortest time frame, they have the following disadvantags:</p>
<ul>
<li>They are <a href="#point-in-time-recommendations">point-in-time recommendations</a> and need to be frequently refreshed to stay current.</li>
<li>They focus on issues for which there are known fixes that can be easily documented, such as the setting of tunable parameters, but not custom fixes to the source code or environment.</li>
</ul>
<h5 id="using-ad-hoc-checklists-correctly"><strong>Using ad hoc checklists correctly</strong> *</h5>
<p>An ad hoc checklist can be an effective way to ensure that everyone knows how to check for the worst of the issues, and that all the obvious culprits have been checked. A checklist can be written to be clear and prescriptive, showing how to identify each issue and what the fix is. However, this list must be constantly updated.</p>
<h4 id="problem-statement">Problem Statement</h4>
<p>Defining the problem statement is a routine task for support staff when first responding to issues. It’s done by asking the customer the following questions:</p>
<ol>
<li>What makes you think there is a performance problem?</li>
<li>Has this system ever performed well?</li>
<li>What changed recently? Software? Hardware? Load?</li>
<li>Can the problem be expressed in terms of latency or runtime?</li>
<li>Does the problem affect other people or applications (or is it just you)?</li>
<li>What is the environment? What software and hardware are used? Versions?  Configuration?</li>
</ol>
<p>Asking and answering these questions often points to an immediate cause and solution. The problem statement has therefore been included here as its own methodology and should be the first approach you use when tackling a new issue.</p>
<h4 id="scientific-method">Scientific Method</h4>
<p>The scientific method studies the unknown by making hypotheses and then testing them. It can be summarized in the following steps:</p>
<ol>
<li>Question: begin with the performance problem statement.</li>
<li>Hypothesis: hypothesize what the cause of poor performance may be.</li>
<li>Prediction: make a prediction based on the hypothesis</li>
<li>Test: construct a test, which may be observational or experimental, that tests the prediction</li>
<li>Analysis: finish with analysis of the test data collected.</li>
</ol>
<p>For example, you may find that application performance is degraded after migrating to a system with less main memory, and you hypothesize that the cause of poor performance is a smaller file system cache. You might use the following tests:</p>
<ul>
<li><strong>Observational test</strong>: measure the cache miss rate on both systems, predicting that cache misses will be higher on the smaller system.</li>
<li><strong>Experimental test</strong> : increase the cache size (adding RAM), predicting that performance will improve.<ul>
<li>Another easier experimental test is to artificially reduce the cache size (using tunable parameters), predicting that performance will be worse.</li>
</ul>
</li>
</ul>
<p>The following are some more examples.</p>
<h5 id="example-observational"><strong>Example (Observational)</strong></h5>
<ol>
<li>Question: What is causing slow database queries?</li>
<li>Hypothesis: Noisy neighbors (other cloud computing tenants) are performing disk I/O, contending with database disk I/O (via the file system).</li>
<li>Prediction: If file system I/O latency is measured during a query, it will show that the file system is responsible for the slow queries.</li>
<li>Test: Tracing of database file system latency as a ratio of query latency shows that less than 5% of the time is spent waiting for the file system.</li>
<li>Analysis: The file system and disks are not responsible for slow queries.</li>
</ol>
<p>Although the issue is still unsolved, some large components of the environment have been ruled out. The person conducting this investigation can return to step 2 and develop a new hypothesis</p>
<h5 id="example-experimental"><strong>Example (Experimental)</strong></h5>
<ol>
<li>Question: Why do HTTP requests take longer from host A to host C than from host B to host C?</li>
<li>Hypothesis: Host A and host B are in different data centers.</li>
<li>Prediction: Moving host A to the same data center as host B will fix the problem.</li>
<li>Test: Move host A and measure performance.</li>
<li>Analysis: Performance has been fixed—consistent with the hypothesis.</li>
</ol>
<p>If the problem wasn’t fixed, reverse the experimental change (move host A back, in this case) before beginning a new hypothesis!</p>
<h5 id="example-experimental_1"><strong>Example (Experimental)</strong></h5>
<ol>
<li>Question: Why did file system performance degrade as the file system cache grew in size?</li>
<li>Hypothesis: A larger cache stores more records, and more compute is required to manage a larger cache than a smaller one.</li>
<li>Prediction: Making the record size progressively smaller, and therefore causing more records to be used to store the same amount of data, will make performance progressively worse.</li>
<li>Test: Test the same workload with progressively smaller record sizes.</li>
<li>Analysis: Results are graphed and are consistent with the prediction. Drilldown analysis is now performed on the cache management routines.</li>
</ol>
<p>This is an example of a negative test: deliberately hurting performance to learn more about the target system.</p>
<h4 id="diagnosis-cycle">Diagnosis Cycle</h4>
<p>The <strong>diagnosis cycle</strong> is this:</p>
<blockquote>
<p>hypothesis → instrumentation → data → hypothesis</p>
</blockquote>
<p>Like the scientific method, this method also deliberately tests a hypothesis through the collection of data. The cycle emphasizes that the data can lead quickly to a new hypothesis, which is tested and refined, and so on. <u>This is similar to a doctor making a series of small tests to diagnose a patient and refining the hypothesis based on the result of each test.</u></p>
<p>Both of these approaches have a good balance of theory and data: try to move from hypothesis to data quickly, so that bad theories can be identified early and discarded, and better ones developed.</p>
<h4 id="tools-method">Tools Method</h4>
<p>A tools-oriented approach is as follows:</p>
<ol>
<li>List available performance tools.</li>
<li>For each tool, list useful metrics it provides.</li>
<li>For each metric, list possible rules for interpretation.</li>
</ol>
<p>The result is a prescriptive checklist of tools, metrics and interpretation. While effective, this method relies exclusively on available (or known) tools, which can provide an incomplete view of the system, similar to the <a href="#streetlight-anti-method">streetlight anti-method</a>; it is worse if users are unaware of this. Issues that require custom tooling (e.g., dynamic tracing) may never be identified and solved.</p>
<h5 id="major-problems-of-the-tools-method"><strong>Major problems of the tools method</strong> *</h5>
<p>In practice, the tools method does identify certain resource bottlenecks, errors, and other types of problems, though often not efficiently.</p>
<p>When a large number of tools and metrics are available, it can be time-consuming to iterate through them. The situation gets worse when multiple tools appear to have the same functionality, and you spend additional time trying to understand the pros and cons of each. In some cases, such as file system micro-benchmark tools, there are over a dozen tools to choose from, when you may need only one.</p>
<h4 id="the-use-method">The USE Method</h4>
<h3 id="modeling">Modeling</h3>
<h3 id="capacity-planning">Capacity Planning</h3>
<h3 id="statistics">Statistics</h3>
<h3 id="monitoring">Monitoring</h3>
<h3 id="visualizations">Visualizations</h3>
<h3 id="doubts-and-solutions">Doubts and Solutions</h3>
<h4 id="verbatim">Verbatim</h4>
<p>p17 on System under Test:</p>
<blockquote>
<p>The mere act of mapping the environment may help to reveal previously overlooked sources of perturbations. The environment may also be modeled as a network of queueing systems, for analytical study.</p>
</blockquote>
<p>WTF?</p>
<p>p21 on Trade-offs:</p>
<blockquote>
<p>File system record size and network buffer size: small vs large</p>
</blockquote>
<p>Further reading may be required to understand these trade-offs.</p>
            </div>
        </div>

        <footer class="col-md-12">
            
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script src="../../js/base.js"></script>
        <script src="../../custom.js"></script>
    </body>
</html>